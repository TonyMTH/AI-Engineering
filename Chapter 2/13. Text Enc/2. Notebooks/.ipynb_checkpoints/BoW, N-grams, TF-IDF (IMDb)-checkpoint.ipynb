{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## EDA\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "########################## Preprecessing\n",
    "#import spacy\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "########################## Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.naive_bayes  import BernoulliNB\n",
    "from sklearn.ensemble     import RandomForestClassifier\n",
    "from xgboost              import XGBClassifier\n",
    "\n",
    "########################## Machine Learning Evaluation\n",
    "from sklearn.metrics         import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "\n",
    "\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in training data: 20000\n",
      "Number of documents in validation data: 5000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"imdb.csv\")\n",
    "\n",
    "df_train = df.sample(frac = 0.8)\n",
    "  \n",
    "df_valid = df.drop(df_train.index)\n",
    "\n",
    "print(\"Number of documents in training data: %d\" % len(df_train))\n",
    "print(\"Number of documents in validation data: %d\" % len(df_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sb.countplot(df_train['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = df[df['sentiment']==0]['review']\n",
    "negative = df[df['sentiment']==1]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Negative')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\n",
    "\n",
    "wordcloud1 = WordCloud( background_color='white',\n",
    "                        width=600,\n",
    "                        height=400).generate(\" \".join(positive))\n",
    "ax1.imshow(wordcloud1)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Positive', fontsize=40);\n",
    "\n",
    "wordcloud2 = WordCloud( background_color='white',\n",
    "                        width=600,\n",
    "                        height=400).generate(\" \".join(negative))\n",
    "ax2.imshow(wordcloud2)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Negative',fontsize=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start with any NLP project we need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Some of the basic text pre-processing techniques includes:\n",
    "\n",
    "- Make text all **lower case** or **upper case** so that the algorithm does not treat the same words in different cases as different\n",
    "- **Removing Noise** i.e everything that isn’t in a standard number or letter i.e Punctuation, Numerical values, common non-sensical text (/n)\n",
    "- **Tokenization**: Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n",
    "- **Stopword Removal**: Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words\n",
    "\n",
    "### More data cleaning steps after tokenization:\n",
    "- **Stemming**: Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form — generally a written word form. Example if we were to stem the following words: “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, the result would be a single word “stem”.\n",
    "- **Lemmatization**: A slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that “run” is a base form for words like “running” or “ran” or that the word “better” and “good” are in the same lemma so they are considered the same.\n",
    "- **Parts of speech** tagging\n",
    "- Create **bi-grams** or tri-grams And more...\n",
    "\n",
    "However, it is not necessary that you would need to use all these steps. The usage depends on your problem at hand. Sometimes removal of stop words helps while at other times, this might not help.Here is a nice table taken from the blog titled: [All you need to know about Text Preprocessing for Machine Learning & NLP](https://kavita-ganesan.com/text-preprocessing-tutorial) that summarizes how much preprocessing you should be performing on your text data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'Hello this is an sentence',\n",
    "    'Hello this is another sentence',\n",
    "    'Hello Hello Hello, this is mellow'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'countVectorizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18300/2787541106.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcountVectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'countVectorizer'"
     ]
    }
   ],
   "source": [
    "countVectorizer = CountVectorizer()\n",
    "\n",
    "texts.countVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (c:\\Users\\Asus\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:301180)",
      "at w.execute (c:\\Users\\Asus\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:300551)",
      "at w.start (c:\\Users\\Asus\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:296215)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (c:\\Users\\Asus\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:310950)",
      "at async t.CellExecutionQueue.start (c:\\Users\\Asus\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:310490)"
     ]
    }
   ],
   "source": [
    "class BOW:\n",
    "    def lowerCase(string):\n",
    "        return string.lowerCase()\n",
    "\n",
    "st = 'Hello Hello Hello, this is mellow'\n",
    "bow = BOW().lowerCase(str)\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9248/1430415362.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtexts_bow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtexts_bow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text 1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Text 2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Text 3\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# create a CountVectorizer instance\n",
    "# fit tranform the text with it\n",
    "# get the features+\n",
    "\n",
    "bow = 0\n",
    "texts_bow = 0\n",
    "tokens = 0\n",
    "pd.DataFrame(data=texts_bow.toarray(), index=['Text 1', 'Text 2', \"Text 3\"], columns=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words (BOW) + ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>es</th>\n",
       "      <th>es otra</th>\n",
       "      <th>es una</th>\n",
       "      <th>esto</th>\n",
       "      <th>esto es</th>\n",
       "      <th>frase</th>\n",
       "      <th>hola</th>\n",
       "      <th>hola esto</th>\n",
       "      <th>hola hola</th>\n",
       "      <th>hola no</th>\n",
       "      <th>no</th>\n",
       "      <th>no vengas</th>\n",
       "      <th>otra</th>\n",
       "      <th>otra frase</th>\n",
       "      <th>sola</th>\n",
       "      <th>una</th>\n",
       "      <th>una frase</th>\n",
       "      <th>vengas</th>\n",
       "      <th>vengas sola</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Text 1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text 2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text 3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        es  es otra  es una  esto  esto es  frase  hola  hola esto  hola hola  \\\n",
       "Text 1   1        0       1     1        1      1     1          1          0   \n",
       "Text 2   1        1       0     1        1      1     1          1          0   \n",
       "Text 3   0        0       0     0        0      0     3          0          2   \n",
       "\n",
       "        hola no  no  no vengas  otra  otra frase  sola  una  una frase  \\\n",
       "Text 1        0   0          0     0           0     0    1          1   \n",
       "Text 2        0   0          0     1           1     0    0          0   \n",
       "Text 3        1   1          1     0           0     1    0          0   \n",
       "\n",
       "        vengas  vengas sola  \n",
       "Text 1       0            0  \n",
       "Text 2       0            0  \n",
       "Text 3       1            1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do the same approach but using a range in the count vectorizer\n",
    "pd.DataFrame(data=texts_bow.toarray(), index=['Text 1', 'Text 2', \"Text 3\"], columns=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "A problem with the Bag of Words approach is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much “informational content”. Also, it will give more weight to longer documents than shorter documents.\n",
    "\n",
    "One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like “the” that are also frequent across all documents are penalized. This approach to scoring is called Term Frequency-Inverse Document Frequency, or TF-IDF for short, where:\n",
    "\n",
    "- **Term Frequency**: is a scoring of the frequency of the word in the current document.\n",
    "- **Inverse Document Frequency**: is a scoring of how rare the word is across documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>es</th>\n",
       "      <th>esto</th>\n",
       "      <th>frase</th>\n",
       "      <th>hola</th>\n",
       "      <th>no</th>\n",
       "      <th>otra</th>\n",
       "      <th>sola</th>\n",
       "      <th>una</th>\n",
       "      <th>vengas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Text 1</th>\n",
       "      <td>0.433067</td>\n",
       "      <td>0.433067</td>\n",
       "      <td>0.433067</td>\n",
       "      <td>0.336315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.569431</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text 2</th>\n",
       "      <td>0.433067</td>\n",
       "      <td>0.433067</td>\n",
       "      <td>0.433067</td>\n",
       "      <td>0.336315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.569431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text 3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715092</td>\n",
       "      <td>0.403585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              es      esto     frase      hola        no      otra      sola  \\\n",
       "Text 1  0.433067  0.433067  0.433067  0.336315  0.000000  0.000000  0.000000   \n",
       "Text 2  0.433067  0.433067  0.433067  0.336315  0.000000  0.569431  0.000000   \n",
       "Text 3  0.000000  0.000000  0.000000  0.715092  0.403585  0.000000  0.403585   \n",
       "\n",
       "             una    vengas  \n",
       "Text 1  0.569431  0.000000  \n",
       "Text 2  0.000000  0.000000  \n",
       "Text 3  0.000000  0.403585  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Same approach for the TfidfVectorizer\n",
    "pd.DataFrame(data=texts_tfidf.toarray(), index=['Text 1', 'Text 2', \"Text 3\"], columns=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Apply to the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.22 s, sys: 27.5 ms, total: 5.25 s\n",
      "Wall time: 5.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bow_1gram = CountVectorizer()\n",
    "\n",
    "train_bow_1gram = 0\n",
    "valid_bow_1gram = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words 1-2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.9 s, sys: 269 ms, total: 17.2 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bow_2grams = 0\n",
    "train_bow_2grams = 0\n",
    "valid_bow_2grams = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.54 s, sys: 9.89 ms, total: 5.55 s\n",
      "Wall time: 5.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_1gram       = 0\n",
    "train_tfidf_1gram = 0\n",
    "valid_tfidf_1gram = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF 1-2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 213 ms, total: 15.5 s\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_2grams      = 0\n",
    "train_tfidf_2grams = 0\n",
    "valid_tfidf_2grams = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW 1gram    (25000, 76496)\n",
      "BOW 2grams   (25000, 1513369)\n",
      "TFIDF 1gram  (25000, 45185)\n",
      "TFIDF 2grams (25000, 433699)\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"BOW 1gram\",   train_bow_1gram, valid_bow_1gram),\n",
    "    (\"BOW 2grams\",  train_bow_2grams, valid_bow_2grams),\n",
    "    (\"TFIDF 1gram\", train_tfidf_1gram, valid_tfidf_1gram),\n",
    "    (\"TFIDF 2grams\",train_tfidf_2grams, valid_tfidf_2grams)\n",
    "]\n",
    "\n",
    "print(\"BOW 1gram   \", train_bow_1gram.shape)\n",
    "print(\"BOW 2grams  \", train_bow_2grams.shape)\n",
    "print(\"TFIDF 1gram \", train_tfidf_1gram.shape)\n",
    "print(\"TFIDF 2grams\", train_tfidf_2grams.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (<1 s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW 1gram \tAccuracy:\t 0.84428 \tTime:\t 0.018774747848510742\n",
      "BOW 2grams \tAccuracy:\t 0.878 \tTime:\t 0.13236117362976074\n",
      "TFIDF 1gram \tAccuracy:\t 0.8604 \tTime:\t 0.014917850494384766\n",
      "TFIDF 2grams \tAccuracy:\t 0.88544 \tTime:\t 0.05532026290893555\n"
     ]
    }
   ],
   "source": [
    "for name, x_train, x_valid in data:\n",
    "    \n",
    "    #Train a MultinomialNB model and meassure the time\n",
    "\n",
    "    a = accuracy_score(y_valid, model.predict(x_valid))\n",
    "    print(name, \"\\tAccuracy:\\t\", a, \"\\tTime:\\t\", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (30s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW 1gram \tAccuracy:\t 0.8856 \tTime:\t 2.3662497997283936\n",
      "BOW 2grams \tAccuracy:\t 0.90076 \tTime:\t 23.640942335128784\n",
      "TFIDF 1gram \tAccuracy:\t 0.89168 \tTime:\t 1.5041086673736572\n",
      "TFIDF 2grams \tAccuracy:\t 0.89688 \tTime:\t 3.6333465576171875\n"
     ]
    }
   ],
   "source": [
    "for name, x_train, x_valid in data:\n",
    "    #Train a LogisticRegression model and meassure the time\n",
    "\n",
    "    print(name, \"\\tAccuracy:\\t\", a, \"\\tTime:\\t\", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (10+90+6+22 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW 1gram \tAccuracy:\t 0.8482 \tTime:\t 13.228048086166382\n",
      "BOW 2grams \tAccuracy:\t 0.85832 \tTime:\t 93.41943311691284\n",
      "TFIDF 1gram \tAccuracy:\t 0.8454 \tTime:\t 9.45107913017273\n",
      "TFIDF 2grams \tAccuracy:\t 0.8552 \tTime:\t 27.58473038673401\n"
     ]
    }
   ],
   "source": [
    "for name, x_train, x_valid in data:\n",
    "    #Train a RandomForestClassifier model and meassure the time\n",
    "    print(name, \"\\tAccuracy:\\t\", a, \"\\tTime:\\t\", t)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f9376a53bea67c1ed5cf17fe4a618ec842560647dcd0f876afebe1c5f60995f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
